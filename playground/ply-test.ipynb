{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c44351f-0e3b-4f66-9150-94d43d9b4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ply.lex as lex\n",
    "__file__ = \"notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "043b6227-9af2-49ba-8693-0e6a62003195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLexer(object):\n",
    "    # List of token names.   This is always required\n",
    "    __file__ = \":class: MyLexer\"\n",
    "    \n",
    "    states = (\n",
    "        (\"preprocessor\", 'exclusive'),\n",
    "    )\n",
    "\n",
    "    \n",
    "    tokens = (\n",
    "        \"PPDirective\",\n",
    "        \"PPAngleQuote\",\n",
    "        \"PPQuote\",\n",
    "        \"PPLineComment\",\n",
    "        \"PPBlockComment\",\n",
    "        \"Identifier\",\n",
    "        \"Float\",\n",
    "        \"Integer\",\n",
    "        \"Newline\",\n",
    "        \"LB\",\n",
    "        \"RB\",\n",
    "        \"Comment\"\n",
    "    )\n",
    "\n",
    "    # Regular expression rules for simple tokens\n",
    "    def t_PPDirective(self, t):\n",
    "        r\"\\#(include|define|undef|ifdef|ifndef|else|endif)\"\n",
    "        t.lexer.push_state(\"preprocessor\")\n",
    "        return t\n",
    "    \n",
    "    t_preprocessor_PPAngleQuote = r\"<([^>\\n]+)>\"\n",
    "    t_preprocessor_PPQuote = r'\"([^\"\\n]+)\"'\n",
    "    t_preprocessor_PPLineComment = r\"//.*\"\n",
    "    t_preprocessor_PPBlockComment = r\"/\\*[^\\n]*\\*/\"\n",
    "    t_ANY_Identifier = r\"[a-zA-Z_][a-zA-Z0-9_]+\"\n",
    "    t_LB  = r'\\['\n",
    "    t_RB  = r'\\]'\n",
    "    t_Comment = r';.*'\n",
    "\n",
    "    # A regular expression rule with some action code\n",
    "    # Note addition of self parameter since we're in a class\n",
    "    def t_ANY_Float(self, t):\n",
    "        r\"[-+]?(?:\\d*\\.\\d+|\\d+\\.\\d*)(?:e[+-]?\\d+)?\"\n",
    "        t.value = float(t.value)\n",
    "        return t\n",
    "\n",
    "    def t_ANY_Integer(self, t):\n",
    "        r'[-+]?\\d+'\n",
    "        t.value = int(t.value)    \n",
    "        return t\n",
    "\n",
    "    # Define a rule so we can track line numbers\n",
    "    def t_preprocessor_Newline(self, t):\n",
    "        r'\\n'\n",
    "        t.lexer.lineno += len(t.value)\n",
    "        t.value = None\n",
    "        t.lexer.pop_state()\n",
    "        return t\n",
    "        \n",
    "    def t_Newline(self, t):\n",
    "        r'\\n'\n",
    "        t.lexer.lineno += len(t.value)\n",
    "        t.value = None\n",
    "        return t\n",
    "\n",
    "    # A string containing ignored characters (spaces and tabs)\n",
    "    t_INITIAL_preprocessor_ignore  = ' \\t'\n",
    "\n",
    "    # Error handling rule\n",
    "    def t_error(self, t):\n",
    "        raise ValueError(f\"Illegal character {t.value[0]} in statement {t.value.splitlines()[0]}\")\n",
    "\n",
    "    def t_preprocessor_error(self, t):\n",
    "        raise ValueError(f\"Illegal character {t.value[0]} in preprocessor statement {t.value.splitlines()[0]}\")\n",
    "\n",
    "    # Build the lexer\n",
    "    def build(self, **kwargs):\n",
    "        self.lexer = lex.lex(module=self, **kwargs)\n",
    "    \n",
    "    # Test it output\n",
    "    def test(self,data):\n",
    "        self.lexer.input(data)\n",
    "        tokens = []\n",
    "        while True:\n",
    "            tok = self.lexer.token()\n",
    "            if not tok: \n",
    "                break\n",
    "            tokens.append(tok)\n",
    "            \n",
    "        return tokens\n",
    "\n",
    "    def tokenize(self, data):\n",
    "        pass\n",
    "\n",
    "\n",
    "def build_lexer(lexer_cls):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    lexer = lexer_cls()\n",
    "    #__file__ = lexer_cls.filename\n",
    "    lexer.build()\n",
    "    return lexer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78ed10c9-d9cb-4a50-af7f-0f4f42095347",
   "metadata": {},
   "source": [
    "def build_lexer(lexer_cls):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    lexer = lexer_cls()\n",
    "    #__file__ = lexer_cls.filename\n",
    "    lexer.build()\n",
    "    return lexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc3a237b-154b-44af-a0f1-d8a6a517a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = build_lexer(MyLexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf05b103-98af-43ed-b618-b75b3c92db12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':class: MyLexer'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87d74243-e121-48e3-a0a1-4651c1188e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"\\\n",
    "; Header\n",
    "#include <iostream> <top>\n",
    "#include \"topol.top\" /* asdf *asdf */\n",
    "#define A123 _a_1_23 \n",
    "[ section ]\n",
    "val _Avl .1 -1 ;// dfgh\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62667958-7381-4bf4-93c3-36a5bb47d9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LexToken(Comment,'; Header',1,0),\n",
       " LexToken(Newline,None,1,8),\n",
       " LexToken(PPDirective,'#include',2,9),\n",
       " LexToken(PPAngleQuote,'<iostream>',2,18),\n",
       " LexToken(PPAngleQuote,'<top>',2,29),\n",
       " LexToken(Newline,None,2,34),\n",
       " LexToken(PPDirective,'#include',3,35),\n",
       " LexToken(PPQuote,'\"topol.top\"',3,44),\n",
       " LexToken(PPBlockComment,'/* asdf *asdf */',3,56),\n",
       " LexToken(Newline,None,3,72),\n",
       " LexToken(PPDirective,'#define',4,73),\n",
       " LexToken(Identifier,'A123',4,81),\n",
       " LexToken(Identifier,'_a_1_23',4,86),\n",
       " LexToken(Newline,None,4,94),\n",
       " LexToken(LB,'[',5,95),\n",
       " LexToken(Identifier,'section',5,97),\n",
       " LexToken(RB,']',5,105),\n",
       " LexToken(Newline,None,5,106),\n",
       " LexToken(Identifier,'val',6,107),\n",
       " LexToken(Identifier,'_Avl',6,111),\n",
       " LexToken(Float,0.1,6,116),\n",
       " LexToken(Integer,-1,6,119),\n",
       " LexToken(Comment,';// dfgh',6,122),\n",
       " LexToken(Newline,None,6,130)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.test(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8babf8-53b9-49ed-a0ad-dba0be692c70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
